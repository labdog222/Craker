[
    {
        "filename": [
            "predict_timeout.py"
        ],
        "path": [
            "/Users/mailiya/documents/github/interviewai/interviewai/llm/predict_timeout.py"
        ],
        "content": [
            "import asyncio\nimport logging\n\nfrom langchain_community.llms.openai import OpenAI\nfrom tiktoken.model import encoding_for_model\n\nfrom interviewai import LoggerMixed\nfrom interviewai.chat.interview_callback import InterviewCallback\n\nTIMEOUT_IN_SECONDS = 1.5\n\n\nclass AITokenTimeout(Exception):\n    pass\n\n\nMODEL_TIMEOUT_IN_SECONDS = {\n    \"gpt-3.5-turbo-16k\": 20,\n    \"gpt-4o-mini\": 20,\n    \"gpt-4\": 60,\n    \"default\": 60,\n}\n# TODO: refactor this logic. this is quite messy\nMODEL_TOKEN = {\n    \"gpt-3.5-turbo-1106\": 16385,\n    \"gpt-3.5-turbo\": 4096,\n    \"gpt-3.5-turbo-16k\": 16385,\n    \"gpt-3.5-turbo-instruct\": 4096,\n    \"gpt-3.5-turbo-0613\": 4096,\n    \"gpt-3.5-turbo-16k-0613\": 16385,\n    \"gpt-3.5-turbo-0301\": 4096,\n    \"text-davinci-003\": 4096,\n    \"text-davinci-002\": 4096,\n    \"code-davinci-002\": 8001,\n    \"text-curie-001\": 2049,\n    \"text-babbage-001\": 2049,\n    \"text-ada-001\": 2049,\n    \"davinci\": 2049,\n    \"curie\": 2049,\n    \"babbage\": 2049,\n    \"ada\": 2049,\n    \"text-moderation-latest\": 32768,\n    \"text-moderation-stable\": 32768,\n    \"babbage-002\": 16384,\n    \"davinci-002\": 16384,\n    \"gpt-4-1106-preview\": 128000,\n    \"gpt-4-vision-preview\": 128000,\n    \"gpt-4\": 8192,\n    \"gpt-4-32k\": 32768,\n    \"gpt-4-0613\": 8192,\n    \"gpt-4-32k-0613\": 32768,\n    \"gpt-4-0314\": 8192,\n    \"gpt-4-32k-0314\": 32768,\n    \"gpt-4o\": 128000,\n    \"gpt-4o-mini\": 128000,\n    \"default\": 8192,\n}\n\n\ndef model_timeout(model) -> int:\n    if model in MODEL_TIMEOUT_IN_SECONDS:\n        return MODEL_TIMEOUT_IN_SECONDS[model]\n    else:\n        return MODEL_TIMEOUT_IN_SECONDS[\"default\"]\n\n\ndef count_token(input, model):\n    if model in MODEL_TOKEN:\n        return len(encoding_for_model(model).encode(input))\n    else:\n        return len(encoding_for_model(\"cl100k_base\").encode(input))\n\n\nclass PredictTimeoutManager:\n    def __init__(self, llm: OpenAI, logger: LoggerMixed) -> None:\n        self.llm = llm\n        self.logger = logger\n        self.callback = None\n        for callback in self.llm.callbacks:\n            if isinstance(callback, InterviewCallback):\n                self.callback = callback\n                break\n        if self.callback is None:\n            logging.info(\"InterviewCallback not found in llm.callbacks\")\n\n    def callback(self, llm: OpenAI = None) -> InterviewCallback:\n        llm = llm or self.llm\n        for callback in llm.callbacks:\n            if isinstance(callback, InterviewCallback):\n                return callback\n\n        raise Exception(\"InterviewCallback not found in llm.callbacks\")\n\n    async def wait_for_ai_token(self, llm=None):\n        \"\"\"\n        Wait for ai token to arrive.\n        \"\"\"\n        self.logger.debug(\n            f\"Waiting for ai token to arrive, current ai_request_first_token_time: {self.callback.ai_request_first_token_time}\"\n        )\n        while self.callback.ai_request_first_token_time is None:\n            await asyncio.sleep(0.1)\n        self.logger.debug(\n            f\"Got a first token from LLM! {self.callback.ai_request_first_token_time}\"\n        )\n        return self.callback.ai_request_first_token_time\n\n    def trim_message_if_exceeded(self, query, model):\n        # consider add datadog monitor within the function\n        if model in MODEL_TOKEN:\n            token_limit = MODEL_TOKEN[model] - 100\n\n        else:\n            token_limit = MODEL_TOKEN[\"default\"] - 100\n        query_count = count_token(query, model)\n        cut_off = len(query) // 2\n        if query_count > token_limit:\n            query_after = query\n            while query_count > token_limit:\n                # trim from the top\n                query_after = query_after[cut_off:]\n                query_count = count_token(query_after, model)\n                cut_off = len(query_after) // 2\n                self.logger.info(f\"Trimming query to {query_count} tokens\")\n            # track\n            track_event(\n                self.logger.user_id,\n                \"token_exceeded\",\n                {\n                    \"model\": model,\n                    \"token_limit\": token_limit,\n                    \"user_id\": self.logger.user_id,\n                    \"query_before\": query,\n                    \"query_after\": query_after,\n                    \"query_before_tokens\": count_token(query, model),\n                    \"query_after_tokens\": count_token(query_after, model),\n                },\n            )\n        else:\n            query_after = query\n        return query_after\n\n    async def apredict(self, query, timeout=TIMEOUT_IN_SECONDS, llm=None):\n        # consider add datadog monitor within the function\n        \"\"\"\n        Call llm.apredict() to get the coroutine and check timeout.\n        Early return if timeout.\n        ai_request_start_time and ai_request_first_token_time will be updated in callback function.\n        1. call self.llm.apredict()\n        2. wait for timeout seconds\n        3. check if timeout\n        4. if timeout, raise AITokenTimeout\n        5. if not timeout, return response\n        \"\"\"\n        llm = llm or self.llm  # use self.llm if llm is not provided\n        timeout_in_seconds = model_timeout(llm.model_name)\n        self.logger.debug(\n            f\"executing apredict with timeout_in_seconds: {timeout_in_seconds}, model: {llm.model_name}\"\n        )\n        # trim message if exceeded\n        query = self.trim_message_if_exceeded(query, llm.model_name)\n        # wrap blocking call llm.predict(query) as async call\n        apredict = asyncio.get_running_loop().run_in_executor(None, llm.predict, query)\n        # pass llm\n        token_arrived_signal = asyncio.create_task(self.wait_for_ai_token(llm))\n        # async await token_arrived_signal and sleep, which ever comes first\n        done, pending = await asyncio.wait(\n            [token_arrived_signal, asyncio.sleep(timeout)],\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n        if token_arrived_signal in done:\n            # token arrived\n            self.logger.debug(\"token arrived, proceed normally\")\n            return await asyncio.wait_for(apredict, timeout=timeout_in_seconds)\n        else:\n            # timeout\n            if self.callback.trace_token_arrived_span:\n                self.callback.trace_token_arrived_span.set_attribute(\"is_timeout\", True)\n                self.callback.trace_token_arrived_span.set_attribute(\n                    \"timeout_seconds\", timeout\n                )\n                self.callback.trace_token_arrived_span.set_attribute(\n                    \"user_id\", self.callback.logger.user_id\n                )\n                self.callback.trace_token_arrived_span.set_attribute(\n                    \"interview_session_id\", self.callback.logger.interview_session_id\n                )\n            raise AITokenTimeout(\n                f\"AI response timeout. AI requested, but takes too long for first token to arrive. timeout limit {timeout} seconds\"\n            )\n"
        ]
    },
    {
        "filename": [
            "embedding.py"
        ],
        "path": [
            "/Users/mailiya/documents/github/interviewai/interviewai/llm/embedding.py"
        ],
        "content": [
            "from typing import List\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom interviewai import LoggerMixed\nfrom interviewai.llm.predict import LLM\n\nlogger = LoggerMixed(__name__)\n\nllm = LLM(logger, model_name=\"text-embedding-ada-002\")\n\n\nclass FROpenAIEmbeddings(OpenAIEmbeddings):\n    \"\"\"\n    Rewrite OpenAIEmbeddings to make it compatible with the latest version of OpenAI API.\n    Retry https://platform.openai.com/docs/guides/error-codes/python-library-error-types\n    Migrated from: `langchain/embeddings/openai.py`\n    \"\"\"\n\n    def _embedding_func(self, text: str, *, engine: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint.\"\"\"\n        return llm.embed(text)\n"
        ]
    },
    {
        "filename": [
            "predict.py"
        ],
        "path": [
            "/Users/mailiya/documents/github/interviewai/interviewai/llm/predict.py"
        ],
        "content": [
            "\"\"\"\nLiteLLM provides a LLM API gateway.\nhttps://github.com/BerriAI/litellm\n\nhttps://litellm.vercel.app/docs/observability/langfuse_integration\n\nGoal:\nWe try to move away from langchain.\n\"\"\"\n\nimport time\nfrom typing import List, Union\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langfuse.openai import openai\nfrom openai import AsyncOpenAI\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom openai.types.create_embedding_response import CreateEmbeddingResponse, Embedding\n\nfrom interviewai import LoggerMixed\nfrom interviewai.core.config import get_config\nfrom interviewai.core.tracer import get_tracer\nfrom interviewai.util import count_token\n\ntracer = get_tracer()\n\n\nclass LLMBase:\n    def __init__(\n        self,\n        logger: LoggerMixed,\n        enable_langfuse=True,\n        model_name=\"gpt-4\",\n        callbacks: Union[bool, List[BaseCallbackHandler]] = False,\n    ):\n        self.model_name = model_name\n        # TODO: utilize allcallback_handler\n        self.callbacks = callbacks\n        self.logger = logger\n        self.logger.debug(\n            f\"initializing LLM with model_name: {model_name}\",\n            user_id=self.logger.user_id,\n            interview_session_id=self.logger.interview_session_id,\n        )\n        if enable_langfuse:\n            self.setup_langfuse()\n        self.client = openai.OpenAI(\n            api_key=get_config(\"LITELLM_API_KEY\"),\n            # api_key=\"sk-NUnBmPCU_vpretlF0IVaiA\",\n            base_url=get_config(\n                \"LITELLM_API_ENDPOINT\"\n            ),  # LiteLLM Proxy is OpenAI compatible, Read More: https://docs.litellm.ai/docs/proxy/user_keys\n        )\n        self.async_client = AsyncOpenAI(\n            api_key=get_config(\"LITELLM_API_KEY\"),\n            # api_key=\"sk-NUnBmPCU_vpretlF0IVaiA\",\n            base_url=get_config(\n                \"LITELLM_API_ENDPOINT\"\n            ),  # LiteLLM Proxy is OpenAI compatible, Read More: https://docs.litellm.ai/docs/proxy/user_keys\n        )\n\n    def setup_langfuse(self):\n        \"\"\"\n        https://langfuse.com/docs/integrations/openai/python/get-started\n        \"\"\"\n        openai.langfuse_public_key = get_config(\"LANGFUSE_PUBLIC_KEY\")\n        openai.langfuse_secret_key = get_config(\"LANGFUSE_PRIVATE_KEY\")\n        openai.langfuse_host = get_config(\"LANGFUSE_HOST\")\n\n\nclass LLM(LLMBase):\n    \"\"\"\n    We will try to follow the API standard of langchain.\n    https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.base.BaseLanguageModel.html#langchain_core.language_models.base.BaseLanguageModel\n    https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM\n    \"\"\"\n\n    def __init__(\n        self,\n        logger: LoggerMixed,\n        enable_langfuse=True,\n        model_name=\"gpt-4\",\n        callbacks: Union[bool, List[BaseCallbackHandler]] = False,\n    ):\n        super().__init__(\n            logger=logger,\n            enable_langfuse=enable_langfuse,\n            model_name=model_name,\n            callbacks=callbacks,\n        )\n\n    def stream(self, response: ChatCompletion) -> str:\n        \"\"\"\n        Streaming responses have a delta field rather than a message field. delta can hold things like:\n            a role token (e.g., {\"role\": \"assistant\"})\n            a content token (e.g., {\"content\": \"\\n\\n\"})\n            nothing (e.g., {}), when the stream is over\n        \"\"\"\n        # record the time before the request is sent\n        start_time = time.time()\n        # create variables to collect the stream of chunks\n        collected_chunks = []\n        collected_messages = []\n        for chunk in response:\n            chunk_time = (\n                time.time() - start_time\n            )  # calculate the time delay of the chunk\n            collected_chunks.append(chunk)  # save the event response\n            chunk_message = chunk.choices[0].delta.content  # extract the message\n            collected_messages.append(chunk_message)  # save the message\n            # _debug_chunk(chunk)\n            if self.callbacks != False:\n                for callback_handler in self.callbacks:\n                    # \u6b64\u65f6\u8bf4\u660e\u4e00\u6b21stream mode LLM\u8bf7\u6c42\u5df2\u7ecf\u5b8c\u6210\n                    if chunk.choices[0].delta.content is None:\n                        callback_handler.on_llm_end(response=None)\n                    else:\n                        # stream\u4ee5token\u7684\u5f62\u5f0f\u7ed9\u51fa\u5185\u5bb9\n                        # \u901a\u8fc7on_llm_new_token\u5c06token\u4f20\u9012\u7ed9callback_handler\n                        # \u53ef\u4ee5\u6d4b\u7b97\u51faLLM\u4ece\u8c03\u7528\u5230\u4ea7\u751f\u7b2c\u4e00\u4e2atoken\u7684\u54cd\u5e94\u65f6\u95f4\n                        callback_handler.on_llm_new_token(\n                            chunk.choices[0].delta.content\n                        )\n            # print(chunk_time)\n\n        # clean None in collected_messages\n        collected_messages = [m for m in collected_messages if m is not None]\n        full_reply_content = \"\".join(collected_messages)\n        return full_reply_content\n\n    def predict(self, prompt, stream=True, model=None):\n        \"\"\"\n        https://cookbook.openai.com/examples/how_to_stream_completions\n        \"\"\"\n        # consider add datadog monitor within the functino\n        if self.callbacks != False:\n            for callback_handler in self.callbacks:\n                callback_handler.on_llm_start(prompts=[\"start\"])\n        response: ChatCompletion = self.client.chat.completions.create(\n            model=model or self.model_name,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            stream=stream,\n            user_id=self.logger.user_id,  # track in langfuse\n            session_id=self.logger.interview_session_id,  # track in langfuse\n            # https://docs.litellm.ai/docs/completion/input#provider-specific-params\n            extra_body={  # pass in any provider-specific param, if not supported by openai,\n                \"user\": self.logger.user_id,  # track in litellm\n                # \"metadata\": { # \ud83d\udc48 use for logging additional params (e.g. to langfuse)\n                #     \"generation_name\": \"ishaan-generation-openai-client\",\n                #     \"generation_id\": \"openai-client-gen-id22\",\n                #     \"trace_id\": \"openai-client-trace-id22\",\n                #     \"trace_user_id\": \"openai-client-user-id2\"\n                # }\n            },\n        )\n        if stream:\n            return self.stream(response)\n        return response.choices[0].message.content\n\n    async def apredict(self, prompt, model=None):\n        response = await self.async_client.chat.completions.create(\n            model=model or self.model_name,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            user_id=self.logger.user_id,\n            session_id=self.logger.interview_session_id,\n            extra_body={\n                \"user\": self.logger.user_id,\n            },\n            stream=False,\n        )\n        return response.choices[0].message.content\n\n    def parse(self, prompt, response_format, model=None):\n        response = self.client.beta.chat.completions.parse(\n            model=model or self.model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format=response_format,\n            temperature=0.6,\n            max_tokens=4096,\n            top_p=1,\n            frequency_penalty=0.1,\n            presence_penalty=0,\n        )\n        print(response.choices[0].message.content)\n\n        return response.choices[0].message.content\n\n    @tracer.start_as_current_span(\"llm-embed\")\n    def embed(self, text, model=\"text-embedding-ada-002\"):\n        response: CreateEmbeddingResponse = self.client.embeddings.create(\n            model=model or self.model_name,\n            input=text,\n        )\n        emb: Embedding = response.data[0]\n        return emb.embedding\n\n    def get_num_tokens(self, prompt):\n        return count_token(prompt)\n\n    def _agenerate(self, prompt, **kwargs):\n        # Implementation of asynchronous generation logic here\n        pass\n\n    def _generate(self, prompt, **kwargs):\n        # Implementation of synchronous generation logic here\n        pass\n\n    def _llm_type(self):\n        # Return the type of LLM this is\n        return \"LiteLLM\"\n\n\ndef _debug_chunk(chunk):\n    print(chunk)\n    print(chunk.choices[0].delta.content)\n    print(\"****************\")\n\n\nasync def main():\n    logger = LoggerMixed(\"test\", user_id=\"user_jay\")\n    llm = LLM(logger=logger)\n    res = await llm.apredict(\"What's the capital of France?\")\n    print(res)\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    python interviewai/llm/predict.py\n    \"\"\"\n\n    logger = LoggerMixed(\"test\", user_id=\"user_jay\")\n    llm = LLM(logger=logger)\n    res = llm.predict(\"What's the capital of France?\", stream=False)\n    print(f\"res: {res}\")\n    res = llm.embed(\"What's the capital of France?\")\n    print(f\"res: {res}\")\n    openai.flush_langfuse()\n\n    # asyncio.run(main())\n"
        ]
    },
    {
        "filename": [
            "__init__.py"
        ],
        "path": [
            "/Users/mailiya/documents/github/interviewai/interviewai/llm/__init__.py"
        ],
        "content": [
            ""
        ]
    }
]